{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyNI4I9+pBbhTYykg2y/gGMR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Anuragpandey2005/Knowledge-Assistant-Local-Retrieval-AI-Agent/blob/main/Foxo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# step 1 :Install required libraries\n"
      ],
      "metadata": {
        "id": "hporY_QR9_G6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence-transformers faiss-cpu pdfplumber transformers python-dotenv requests reportlab\n"
      ],
      "metadata": {
        "id": "M5nqxUCz8CUY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2: Import dependencies"
      ],
      "metadata": {
        "id": "8d2d_C6A-HBV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import pdfplumber\n",
        "import numpy as np\n",
        "import faiss\n",
        "from dotenv import load_dotenv\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n"
      ],
      "metadata": {
        "id": "8p0kd-va8s1H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3: Configuration"
      ],
      "metadata": {
        "id": "oDJzFJLf-MaX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Configuration\n",
        "load_dotenv()\n",
        "DOCS_DIR = \"foxo_docs\"\n",
        "CHUNK_SIZE = 300\n",
        "CHUNK_OVERLAP = 50\n",
        "SIMILARITY_THRESHOLD = 0.6"
      ],
      "metadata": {
        "id": "lf5DKGEW80ps"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 4: Enhanced document creation"
      ],
      "metadata": {
        "id": "NaNKLsGU-Rbb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_sample_documents():\n",
        "    os.makedirs(DOCS_DIR, exist_ok=True)\n",
        "\n",
        "    # PDF with multiple pages\n",
        "    from reportlab.lib.pagesizes import letter\n",
        "    from reportlab.platypus import SimpleDocTemplate, Paragraph\n",
        "    from reportlab.lib.styles import getSampleStyleSheet\n",
        "\n",
        "    def create_pdf(filename, content):\n",
        "        doc = SimpleDocTemplate(filename, pagesize=letter)\n",
        "        styles = getSampleStyleSheet()\n",
        "        story = [Paragraph(page, styles[\"Normal\"]) for page in content]\n",
        "        doc.build(story)\n",
        "\n",
        "    create_pdf(\n",
        "        f\"{DOCS_DIR}/security.pdf\",\n",
        "        [\n",
        "            \"Security Policy\\n• AES-256 encryption\\n• Mandatory 2FA for all access\",\n",
        "            \"Data Protection\\n• Daily backups\\n• GDPR compliance standards\"\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    with open(f\"{DOCS_DIR}/benefits.txt\", \"w\") as f:\n",
        "        f.write(\"Employee Benefits:\\n- 25 vacation days/year\\n- Comprehensive health insurance\")"
      ],
      "metadata": {
        "id": "CHUEXqwc86Cs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 5: Improved document loading with page tracking"
      ],
      "metadata": {
        "id": "12Asqeuw-WSW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Improved document loading with page tracking\n",
        "def load_documents():\n",
        "    documents = []\n",
        "    for filename in os.listdir(DOCS_DIR):\n",
        "        path = os.path.join(DOCS_DIR, filename)\n",
        "        if filename.endswith(\".txt\"):\n",
        "            with open(path, \"r\") as f:\n",
        "                documents.append((f.read(), filename, 1))\n",
        "        elif filename.endswith(\".pdf\"):\n",
        "            with pdfplumber.open(path) as pdf:\n",
        "                for page_num, page in enumerate(pdf.pages, 1):\n",
        "                    text = page.extract_text() or \"\"\n",
        "                    documents.append((text, filename, page_num))\n",
        "    return documents"
      ],
      "metadata": {
        "id": "D7o9udln9CTW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 6: Better text splitting with overlap"
      ],
      "metadata": {
        "id": "nYhhQ1Sp-be7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def split_documents(documents):\n",
        "    chunks = []\n",
        "    for content, filename, page in documents:\n",
        "        words = content.split()\n",
        "        for i in range(0, len(words), CHUNK_SIZE - CHUNK_OVERLAP):\n",
        "            chunk = \" \".join(words[i:i+CHUNK_SIZE])\n",
        "            chunks.append((chunk, f\"{filename} (page {page})\"))\n",
        "    return chunks"
      ],
      "metadata": {
        "id": "L2pJ-mrf9Fjb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 7: Embedding system"
      ],
      "metadata": {
        "id": "W_FXbQr--gmk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def create_vector_store(chunks):\n",
        "    embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "    embeddings = embedder.encode([text for text, _ in chunks])\n",
        "\n",
        "    index = faiss.IndexFlatL2(embeddings.shape[1])\n",
        "    index.add(np.array(embeddings, dtype=\"float32\"))\n",
        "    return index, embedder"
      ],
      "metadata": {
        "id": "nZl14GOq9JUI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 8: Retrieval with threshold"
      ],
      "metadata": {
        "id": "Vyi5kqiM-kCt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def retrieve(query, index, embedder, chunks, k=3):\n",
        "    query_embed = embedder.encode([query])\n",
        "    distances, indices = index.search(query_embed.astype(\"float32\"), k)\n",
        "\n",
        "    results = []\n",
        "    for i, score in zip(indices[0], distances[0]):\n",
        "        if score < SIMILARITY_THRESHOLD:\n",
        "            results.append((chunks[i][0], chunks[i][1]))\n",
        "\n",
        "    return results if results else [chunks[indices[0][0]]]\n"
      ],
      "metadata": {
        "id": "F79N__xR9TWf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 9: LLM Setup for T5"
      ],
      "metadata": {
        "id": "F84qouEf-qgc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def setup_llm():\n",
        "    from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
        "\n",
        "    tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-base\")\n",
        "    model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-base\")\n",
        "\n",
        "    return pipeline(\n",
        "        \"text2text-generation\",\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        max_length=200,\n",
        "        temperature=0.3,\n",
        "        repetition_penalty=1.2\n",
        "    )\n"
      ],
      "metadata": {
        "id": "e_kLyBd89UxN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 10: RAG Pipeline\n"
      ],
      "metadata": {
        "id": "tXSb0JYS-5Az"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def answer_question(query, index, embedder, chunks, qa_pipeline):\n",
        "\n",
        "    if \"weather\" in query.lower():\n",
        "        return get_weather(query.split()[-1])\n",
        "    if any(word in query for word in [\"calculate\", \"math\", \"solve\"]):\n",
        "        return calculate(query)\n",
        "\n",
        "    # Document retrieval\n",
        "    docs = retrieve(query, index, embedder, chunks)\n",
        "\n",
        "    # Answer generation\n",
        "    context = \"\\n\".join([f\"[Source {i+1}: {fn}]\\n{text}\"\n",
        "                       for i, (text, fn) in enumerate(docs)])\n",
        "\n",
        "    prompt = f\"\"\"Answer this question using only the context. Cite sources:\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question: {query}\n",
        "Answer:\"\"\"\n",
        "\n",
        "    try:\n",
        "        response = qa_pipeline(prompt, max_length=200)[0]['generated_text']\n",
        "        return response.strip()\n",
        "    except Exception as e:\n",
        "        return f\"Error generating answer: {str(e)}\"\n"
      ],
      "metadata": {
        "id": "hGtKQT8f9dvt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 11: Final RAG pipeline"
      ],
      "metadata": {
        "id": "Y6e9BM19_BtM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def answer_question(query, index, embedder, chunks, qa_pipeline):\n",
        "\n",
        "    if \"weather\" in query.lower():\n",
        "        return get_weather(query.split()[-1])\n",
        "    if any(word in query for word in [\"calculate\", \"math\", \"solve\"]):\n",
        "        return calculate(query)\n",
        "\n",
        "    # Document retrieval\n",
        "    docs = retrieve(query, index, embedder, chunks)\n",
        "\n",
        "    # Answer generation\n",
        "    context = \"\\n\".join([f\"[Source {i+1}: {fn}]\\n{text}\"\n",
        "                       for i, (text, fn) in enumerate(docs)])\n",
        "\n",
        "    prompt = f\"\"\"Use this context to answer. Cite sources like [Source 1].\n",
        "If unsure, say \"I don't know\".\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question: {query}\n",
        "Answer:\"\"\"\n",
        "\n",
        "    try:\n",
        "        response = qa_pipeline(prompt)[0]['generated_text']\n",
        "        return response.split(\"Answer:\")[-1].strip()\n",
        "    except Exception as e:\n",
        "        return f\"Error generating answer: {str(e)}\""
      ],
      "metadata": {
        "id": "1jqh3Qtn9i6G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 12: Main execution"
      ],
      "metadata": {
        "id": "feJO0gzV_Gzo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    create_sample_documents()\n",
        "    raw_docs = load_documents()\n",
        "    chunks = split_documents(raw_docs)\n",
        "    index, embedder = create_vector_store(chunks)\n",
        "    qa_pipeline = setup_llm()\n",
        "\n",
        "    # CLI Interface\n",
        "    print(\"🦊 FOXO Knowledge Assistant\\n\")\n",
        "    while True:\n",
        "        try:\n",
        "            query = input(\"Your question (type 'exit' to quit): \")\n",
        "            if query.lower() in [\"exit\", \"quit\"]:\n",
        "                break\n",
        "\n",
        "            answer = answer_question(query, index, embedder, chunks, qa_pipeline)\n",
        "            print(f\"\\nAssistant: {answer}\\n{'─'*50}\\n\")\n",
        "\n",
        "        except KeyboardInterrupt:\n",
        "            break"
      ],
      "metadata": {
        "id": "Ep9tltAC9oVl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jPMtiopjbhPS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pWl7JiRWfqSO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}